{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmo\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import models as mo\n",
    "import WordMetrics\n",
    "import WordMatching as wm\n",
    "import epitran\n",
    "import ModelInterfaces as mi\n",
    "import AIModels\n",
    "import RuleBasedModels\n",
    "from string import punctuation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-29 18:20:12 nemo_logging:393] <>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:12 nemo_logging:393] <>:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:12 nemo_logging:393] <>:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:12 nemo_logging:393] <>:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:12 nemo_logging:393] /tmp/ipykernel_178061/2047731044.py:19: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "      assert (\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:12 nemo_logging:393] /tmp/ipykernel_178061/2047731044.py:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "      assert (\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import ModelInterfaces\n",
    "import torch\n",
    "import numpy as np\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "\n",
    "class NeuralASR(ModelInterfaces.IASRModel):\n",
    "    word_locations_in_samples = None\n",
    "    audio_transcript = None\n",
    "\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def getTranscript(self) -> str:\n",
    "        \"\"\"Get the transcripts of the process audio\"\"\"\n",
    "        assert (\n",
    "            self.audio_transcript != None,\n",
    "            \"Can get audio transcripts without having processed the audio\",\n",
    "        )\n",
    "        return self.audio_transcript\n",
    "\n",
    "    def getWordLocations(self) -> list:\n",
    "        \"\"\"Get the pair of words location from audio\"\"\"\n",
    "        assert (\n",
    "            self.word_locations_in_samples != None,\n",
    "            \"Can get word locations without having processed the audio\",\n",
    "        )\n",
    "\n",
    "        return self.word_locations_in_samples\n",
    "\n",
    "    def processAudio(self, audio: str):\n",
    "        \"\"\"Process the audio\"\"\"\n",
    "        decoding_cfg = self.model.cfg.decoding\n",
    "        with open_dict(decoding_cfg):\n",
    "            decoding_cfg.preserve_alignments = True\n",
    "            decoding_cfg.compute_timestamps = True\n",
    "            self.model.change_decoding_strategy(decoding_cfg)\n",
    "\n",
    "        hypotheses = self.model.transcribe([audio], return_hypotheses=True)\n",
    "        if type(hypotheses) == tuple and len(hypotheses) == 2:\n",
    "            hypotheses = hypotheses[0]\n",
    "\n",
    "        timestamp_dict = hypotheses[0].timestep\n",
    "        time_stride = 8 * self.model.cfg.preprocessor.window_stride\n",
    "        word_timestamps = timestamp_dict[\"word\"]\n",
    "\n",
    "        for stamp in word_timestamps:\n",
    "            stamp[\"start_ts\"] = stamp.pop(\"start_offset\") * time_stride\n",
    "            stamp[\"end_ts\"] = stamp.pop(\"end_offset\") * time_stride\n",
    "\n",
    "        self.word_locations_in_samples = word_timestamps\n",
    "        self.audio_transcript = hypotheses[0].text\n",
    "\n",
    "\n",
    "class NeuralTTS(ModelInterfaces.ITextToSpeechModel):\n",
    "    def __init__(self, model: torch.nn.Module, sampling_rate: int) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def getAudioFromSentence(self, sentence: str) -> np.array:\n",
    "        with torch.inference_mode():\n",
    "            audio_transcript = self.model.apply_tts(\n",
    "                texts=[sentence], sample_rate=self.sampling_rate\n",
    "            )[0]\n",
    "\n",
    "        return audio_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import models as mo\n",
    "import torchaudio\n",
    "import WordMetrics\n",
    "import WordMatching as wm\n",
    "import epitran\n",
    "import ModelInterfaces as mi\n",
    "import AIModels\n",
    "import RuleBasedModels\n",
    "from string import punctuation\n",
    "import time\n",
    "\n",
    "\n",
    "class PronunciationTrainer:\n",
    "    current_transcript: str\n",
    "    current_ipa: str\n",
    "    current_recorded_audio: torch.Tensor\n",
    "    current_recorded_transcript: str\n",
    "    current_recorded_word_locations: list\n",
    "    current_recorded_intonations: torch.tensor\n",
    "    current_words_pronunciation_accuracy = []\n",
    "    categories_thresholds = np.array([80, 60, 59])\n",
    "\n",
    "    sampling_rate = 16000\n",
    "\n",
    "    def __init__(\n",
    "        self, asr_model: mi.IASRModel, word_to_ipa_coverter: mi.ITextToPhonemModel\n",
    "    ) -> None:\n",
    "        self.asr_model = asr_model\n",
    "        self.ipa_converter = word_to_ipa_coverter\n",
    "\n",
    "    def getTranscriptAndWordsLocations(self, audio_length_in_samples: int):\n",
    "        audio_transcript = self.asr_model.getTranscript()\n",
    "        word_locations_in_samples = self.asr_model.getWordLocations()\n",
    "        # print(\"ASR transcript-output: \", audio_transcript)\n",
    "        # print(\"ASR word-locations: \", word_locations_in_samples)\n",
    "\n",
    "        return audio_transcript, word_locations_in_samples\n",
    "\n",
    "    ##################### ASR Functions ###########################\n",
    "\n",
    "    def processAudioForGivenText(self, recordedAudio: str = None, real_text=None):\n",
    "        start = time.time()\n",
    "        recording_transcript, recording_ipa, word_locations = self.getAudioTranscript(\n",
    "            recordedAudio\n",
    "        )\n",
    "        print(recording_ipa)\n",
    "        print(\"Time for NN to transcript audio: \", str(time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "        (\n",
    "            real_and_transcribed_words,\n",
    "            real_and_transcribed_words_ipa,\n",
    "            mapped_words_indices,\n",
    "        ) = self.matchSampleAndRecordedWords(real_text, recording_transcript)\n",
    "        print(\"Time for matching transcripts: \", str(time.time() - start))\n",
    "\n",
    "        start_time, end_time = self.getWordLocationsFromRecordInSeconds(\n",
    "            word_locations, mapped_words_indices\n",
    "        )\n",
    "        pronunciation_accuracy, current_words_pronunciation_accuracy = (\n",
    "            self.getPronunciationAccuracy(real_and_transcribed_words)\n",
    "        )  # _ipa\n",
    "        pronunciation_categories = self.getWordsPronunciationCategory(\n",
    "            current_words_pronunciation_accuracy\n",
    "        )\n",
    "\n",
    "        result = {\n",
    "            \"recording_transcript\": recording_transcript,\n",
    "            \"real_and_transcribed_words\": real_and_transcribed_words,\n",
    "            \"recording_ipa\": recording_ipa,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"real_and_transcribed_words_ipa\": real_and_transcribed_words_ipa,\n",
    "            \"pronunciation_accuracy\": pronunciation_accuracy,\n",
    "            \"pronunciation_categories\": pronunciation_categories,\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    def getAudioTranscript(self, recordedAudio: str = None):\n",
    "        current_recorded_audio, sample_rate = torchaudio.load(recordedAudio)\n",
    "        transform = torchaudio.transforms.Resample(\n",
    "            orig_freq=sample_rate, new_freq=16000\n",
    "        )\n",
    "        current_recorded_audio = transform(current_recorded_audio)\n",
    "        current_recorded_audio = self.preprocessAudio(current_recorded_audio)\n",
    "\n",
    "        self.asr_model.processAudio(recordedAudio)\n",
    "        current_recorded_transcript, current_recorded_word_locations = (\n",
    "            self.getTranscriptAndWordsLocations(recordedAudio)\n",
    "        )\n",
    "        current_recorded_ipa = self.ipa_converter.convertToPhonem(\n",
    "            current_recorded_transcript\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            current_recorded_transcript,\n",
    "            current_recorded_ipa,\n",
    "            current_recorded_word_locations,\n",
    "        )\n",
    "\n",
    "    def getWordLocationsFromRecordInSeconds(\n",
    "        self, word_locations, mapped_words_indices\n",
    "    ) -> list:\n",
    "        start_time = []\n",
    "        end_time = []\n",
    "        for word_idx in range(len(mapped_words_indices)):\n",
    "            start_time.append(\n",
    "                float(word_locations[mapped_words_indices[word_idx]][\"start_ts\"])\n",
    "            )\n",
    "            end_time.append(\n",
    "                float(word_locations[mapped_words_indices[word_idx]][\"end_ts\"])\n",
    "            )\n",
    "\n",
    "        return \" \".join([str(time) for time in start_time]), \" \".join(\n",
    "            [str(time) for time in end_time]\n",
    "        )\n",
    "\n",
    "    ##################### END ASR Functions ###########################\n",
    "\n",
    "    ##################### Evaluation Functions ###########################\n",
    "\n",
    "    def matchSampleAndRecordedWords(self, real_text, recorded_transcript):\n",
    "        words_estimated = recorded_transcript.split()\n",
    "\n",
    "        if real_text is None:\n",
    "            words_real = self.current_transcript.split()\n",
    "        else:\n",
    "            words_real = real_text.split()\n",
    "            \n",
    "\n",
    "        mapped_words, mapped_words_indices = wm.get_best_mapped_words(\n",
    "            words_estimated, words_real\n",
    "        )\n",
    "        # print(mapped_words, words_estimated, words_real)\n",
    "        real_and_transcribed_words = []\n",
    "        real_and_transcribed_words_ipa = []\n",
    "        for word_idx in range(len(words_real)):\n",
    "            if word_idx >= len(mapped_words) - 1:\n",
    "                mapped_words.append(\"-\")\n",
    "\n",
    "            real_and_transcribed_words.append(\n",
    "                (words_real[word_idx], mapped_words[word_idx])\n",
    "            )\n",
    "            real_and_transcribed_words_ipa.append(\n",
    "                (\n",
    "                   self.ipa_converter.convertToPhonem(words_real[word_idx]), \n",
    "                   self.ipa_converter.convertToPhonem(mapped_words[word_idx]),\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        print(real_and_transcribed_words_ipa)\n",
    "\n",
    "        return (\n",
    "            real_and_transcribed_words,\n",
    "            real_and_transcribed_words_ipa,\n",
    "            mapped_words_indices,\n",
    "        )\n",
    "\n",
    "    def getPronunciationAccuracy(self, real_and_transcribed_words_ipa) -> float:\n",
    "        total_mismatches = 0.0\n",
    "        number_of_phonemes = 0.0\n",
    "        current_words_pronunciation_accuracy = []\n",
    "        for pair in real_and_transcribed_words_ipa:\n",
    "\n",
    "            real_without_punctuation = self.removePunctuation(pair[0]).lower()\n",
    "            number_of_word_mismatches = WordMetrics.edit_distance_python(\n",
    "                real_without_punctuation, self.removePunctuation(pair[1]).lower()\n",
    "            )\n",
    "            total_mismatches += number_of_word_mismatches\n",
    "            number_of_phonemes_in_word = len(real_without_punctuation)\n",
    "            number_of_phonemes += number_of_phonemes_in_word\n",
    "\n",
    "            current_words_pronunciation_accuracy.append(\n",
    "                float(number_of_phonemes_in_word - number_of_word_mismatches)\n",
    "                / number_of_phonemes_in_word\n",
    "                * 100\n",
    "            )\n",
    "\n",
    "        percentage_of_correct_pronunciations = (\n",
    "            (number_of_phonemes - total_mismatches) / number_of_phonemes * 100\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            np.round(percentage_of_correct_pronunciations),\n",
    "            current_words_pronunciation_accuracy,\n",
    "        )\n",
    "\n",
    "    def removePunctuation(self, word: str) -> str:\n",
    "        return \"\".join([char for char in word if char not in punctuation])\n",
    "\n",
    "    def getWordsPronunciationCategory(self, accuracies) -> list:\n",
    "        categories = []\n",
    "\n",
    "        for accuracy in accuracies:\n",
    "            categories.append(self.getPronunciationCategoryFromAccuracy(accuracy))\n",
    "\n",
    "        return categories\n",
    "\n",
    "    def getPronunciationCategoryFromAccuracy(self, accuracy) -> int:\n",
    "        return np.argmin(abs(self.categories_thresholds - accuracy))\n",
    "\n",
    "    def preprocessAudio(self, audio: torch.tensor) -> torch.tensor:\n",
    "        audio = audio - torch.mean(audio)\n",
    "        audio = audio / torch.max(torch.abs(audio))\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-29 18:20:14 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-29 18:20:14 nemo_logging:393] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:14 nemo_logging:393] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:14 nemo_logging:393] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-29 18:20:14 nemo_logging:381] PADDING: 0\n",
      "[NeMo I 2024-05-29 18:20:16 nemo_logging:381] Model EncDecCTCModelBPE was successfully restored from /home/ubuntu/.cache/huggingface/hub/models--nvidia--stt_en_fastconformer_ctc_large/snapshots/42b3eb6bd6f86465f0691f9ea33ddf8f4c5d1c10/stt_en_fastconformer_ctc_large.nemo.\n"
     ]
    }
   ],
   "source": [
    "def getTrainer(language: str):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = mo.getASRModel2()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    asr_model = NeuralASR(model)\n",
    "\n",
    "    if language == \"de\":\n",
    "        phonem_converter = RuleBasedModels.EpitranPhonemConverter(\n",
    "            epitran.Epitran(\"eng-Latn\")\n",
    "        )\n",
    "    elif language == \"en\":\n",
    "        phonem_converter = RuleBasedModels.EngPhonemConverter()\n",
    "    else:\n",
    "        raise ValueError(\"Language not implemented\")\n",
    "\n",
    "    trainer = PronunciationTrainer(asr_model, phonem_converter)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "trainer = getTrainer(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-29 18:20:16 nemo_logging:381] Changed decoding strategy to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: true\n",
      "    compute_timestamps: true\n",
      "    word_seperator: ' '\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg: null\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: 0.33\n",
      "    temperature: 1.0\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2abdb29c8746dc9cc9318bd21812a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-29 18:20:17 nemo_logging:393] /opt/conda/envs/testing/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "      warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prɪntɪŋ ɪn ðə oʊnli sɛns wɪð wɪtʃ wiː ɑːr æt prɛzənt kəlsɑːnuː dɪfɝz frʌm moʊst ɪf nɑːt frʌm ɔl ɑːrtən ənd tʃɛrəftɝ rɛprəzɛntəd ɪn ætʃɪgɪsən\n",
      "Time for NN to transcript audio:  0.9944329261779785\n",
      "[('prɪntɪŋ', 'prɪntɪŋ'), ('ɪn', 'ɪn'), ('ðə', 'ðə'), ('oʊnli', 'oʊnli'), ('sɛns', 'sɛns'), ('wɪð', 'wɪð'), ('wɪtʃ', 'wɪtʃ'), ('wiː', 'wiː'), ('ɑːr', 'ɑːr'), ('æt', 'æt'), ('prɛzənt', 'prɛzənt'), ('kənsɝːnd', 'kəlsɑːnuː'), ('dɪfɝz', 'dɪfɝz'), ('frʌm', 'frʌm'), ('moʊst', 'moʊst'), ('ɪf', 'ɪf'), ('nɑːt', 'nɑːt'), ('frʌm', 'frʌm'), ('ɔl', 'ɔl'), ('ðə', '-'), ('ɑːrts', 'ɑːrtən'), ('ənd', 'ənd'), ('kræfts', 'tʃɛrəftɝ'), ('rɛprəzɛntəd', 'rɛprəzɛntəd'), ('ɪn', 'ɪn'), ('ðə', '-'), ('ɛksəbɪʃən', 'ætʃɪgɪsən')]\n",
      "Time for matching transcripts:  8.331892251968384\n"
     ]
    }
   ],
   "source": [
    "real_transcript = \"Printing in the only sense with which we are at present concerned differs from most if not from all the arts and crafts represented in the Exhibition\"\n",
    "file_path = \"output.wav\"\n",
    "\n",
    "result = trainer.processAudioForGivenText(file_path, real_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recording_transcript': 'printing in the only sense with which we are at present callsanu differs from most if not from all artan and charaafter represented in achigisson', 'real_and_transcribed_words': [('Printing', 'printing'), ('in', 'in'), ('the', 'the'), ('only', 'only'), ('sense', 'sense'), ('with', 'with'), ('which', 'which'), ('we', 'we'), ('are', 'are'), ('at', 'at'), ('present', 'present'), ('concerned', 'callsanu'), ('differs', 'differs'), ('from', 'from'), ('most', 'most'), ('if', 'if'), ('not', 'not'), ('from', 'from'), ('all', 'all'), ('the', '-'), ('arts', 'artan'), ('and', 'and'), ('crafts', 'charaafter'), ('represented', 'represented'), ('in', 'in'), ('the', '-'), ('Exhibition', 'achigisson')], 'recording_ipa': 'prɪntɪŋ ɪn ðə oʊnli sɛns wɪð wɪtʃ wiː ɑːr æt prɛzənt kəlsɑːnuː dɪfɝz frʌm moʊst ɪf nɑːt frʌm ɔl ɑːrtən ənd tʃɛrəftɝ rɛprəzɛntəd ɪn ætʃɪgɪsən', 'start_time': '0.72 1.2 1.36 1.52 1.68 2.16 2.64 2.8000000000000003 3.04 3.2 3.44 3.6 4.72 5.68 5.92 6.16 6.640000000000001 6.96 7.28 10.96 7.5200000000000005 8.24 8.4 9.040000000000001 10.8 10.96 10.96', 'end_time': '1.2 1.36 1.52 1.68 2.16 2.64 2.8000000000000003 3.04 3.2 3.44 3.6 4.72 5.68 5.92 6.16 6.640000000000001 6.96 7.28 7.5200000000000005 11.92 8.24 8.4 9.040000000000001 10.8 10.96 11.92 11.92', 'real_and_transcribed_words_ipa': [('prɪntɪŋ', 'prɪntɪŋ'), ('ɪn', 'ɪn'), ('ðə', 'ðə'), ('oʊnli', 'oʊnli'), ('sɛns', 'sɛns'), ('wɪð', 'wɪð'), ('wɪtʃ', 'wɪtʃ'), ('wiː', 'wiː'), ('ɑːr', 'ɑːr'), ('æt', 'æt'), ('prɛzənt', 'prɛzənt'), ('kənsɝːnd', 'kəlsɑːnuː'), ('dɪfɝz', 'dɪfɝz'), ('frʌm', 'frʌm'), ('moʊst', 'moʊst'), ('ɪf', 'ɪf'), ('nɑːt', 'nɑːt'), ('frʌm', 'frʌm'), ('ɔl', 'ɔl'), ('ðə', '-'), ('ɑːrts', 'ɑːrtən'), ('ənd', 'ənd'), ('kræfts', 'tʃɛrəftɝ'), ('rɛprəzɛntəd', 'rɛprəzɛntəd'), ('ɪn', 'ɪn'), ('ðə', '-'), ('ɛksəbɪʃən', 'ætʃɪgɪsən')], 'pronunciation_accuracy': 80.0, 'pronunciation_categories': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronunciation Score:  80.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pronunciation Score: \", result[\"pronunciation_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_tts = {\n",
    "    \"body\": json.dumps(\n",
    "        {\n",
    "            \"value\": \"Printing in the only sense with which we are at present concerned differs from most if not from all the arts and crafts represented in the Exhibition\",\n",
    "            \"language\": \"de\",\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing in the only sense with which we are at present concerned differs from most if not from all the arts and crafts represented in the Exhibition\n"
     ]
    }
   ],
   "source": [
    "real_transcript = json.loads(event_tts[\"body\"])[\"value\"]\n",
    "print(real_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/snakers4_silero-models_master\n",
      "[NeMo W 2024-05-29 18:20:28 nemo_logging:393] <torch_package_0>.mono_v2_package.py:22: UserWarning: Text string is longer than 140 symbols.\n",
      "      warnings.warn('Text string is longer than 140 symbols.')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UklGRqSjBABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACABAAZGF0YYCjBAATAC0ANgA/AE8AUQBbAGUAbQBpAGYAYwBaAE8ASABC\n"
     ]
    }
   ],
   "source": [
    "import lambdaTTS\n",
    "\n",
    "output_tts = lambdaTTS.lambda_handler(event_tts, [])\n",
    "\n",
    "body = json.loads(output_tts[\"body\"])\n",
    "wavBase64 = body[\"wavBase64\"]\n",
    "print(wavBase64[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_audio = wavBase64\n",
    "\n",
    "dummy_event = {\n",
    "    \"body\": json.dumps(\n",
    "        {\n",
    "            \"title\": real_transcript,\n",
    "            \"base64Audio\": \"data:audio/ogg;base64,\" + encoded_audio,\n",
    "            \"language\": \"de\",\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-29 18:20:32 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-29 18:20:32 nemo_logging:393] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:32 nemo_logging:393] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:32 nemo_logging:393] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-29 18:20:32 nemo_logging:381] PADDING: 0\n",
      "[NeMo I 2024-05-29 18:20:34 nemo_logging:381] Model EncDecCTCModelBPE was successfully restored from /home/ubuntu/.cache/huggingface/hub/models--nvidia--stt_en_fastconformer_ctc_large/snapshots/42b3eb6bd6f86465f0691f9ea33ddf8f4c5d1c10/stt_en_fastconformer_ctc_large.nemo.\n",
      "[NeMo I 2024-05-29 18:20:36 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-29 18:20:36 nemo_logging:393] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:36 nemo_logging:393] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2024-05-29 18:20:36 nemo_logging:393] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-29 18:20:36 nemo_logging:381] PADDING: 0\n",
      "[NeMo I 2024-05-29 18:20:38 nemo_logging:381] Model EncDecCTCModelBPE was successfully restored from /home/ubuntu/.cache/huggingface/hub/models--nvidia--stt_en_fastconformer_ctc_large/snapshots/42b3eb6bd6f86465f0691f9ea33ddf8f4c5d1c10/stt_en_fastconformer_ctc_large.nemo.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import WordMatching as wm\n",
    "import utilsFileIO\n",
    "import pronunciationTrainer\n",
    "import base64\n",
    "import time\n",
    "import audioread\n",
    "import numpy as np\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "\n",
    "trainer_SST_lambda = {}\n",
    "trainer_SST_lambda[\"de\"] = getTrainer(\"de\")\n",
    "trainer_SST_lambda[\"en\"] = getTrainer(\"en\")\n",
    "\n",
    "transform = Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "\n",
    "def lambda_handler(event, result):\n",
    "\n",
    "    data = json.loads(event[\"body\"])\n",
    "\n",
    "    real_text = data[\"title\"]\n",
    "    file_bytes = base64.b64decode(data[\"base64Audio\"][22:].encode(\"utf-8\"))\n",
    "    language = data[\"language\"]\n",
    "\n",
    "    if len(real_text) == 0:\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"headers\": {\n",
    "                \"Access-Control-Allow-Headers\": \"*\",\n",
    "                \"Access-Control-Allow-Credentials\": \"true\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"OPTIONS,POST,GET\",\n",
    "            },\n",
    "            \"body\": \"\",\n",
    "        }\n",
    "\n",
    "    start = time.time()\n",
    "    random_file_name = \"./\" + utilsFileIO.generateRandomString() + \".ogg\"\n",
    "    f = open(random_file_name, \"wb\")\n",
    "    f.write(file_bytes)\n",
    "    f.close()\n",
    "    print(\"Time for saving binary in file: \", str(time.time() - start))\n",
    "\n",
    "    # start = time.time()\n",
    "    # signal, fs = audioread_load(random_file_name)\n",
    " \n",
    "    # signal = transform(torch.Tensor(signal)).unsqueeze(0)\n",
    "\n",
    "    print(\"Time for loading .ogg file file: \", str(time.time() - start))\n",
    "\n",
    "    result = trainer_SST_lambda[language].processAudioForGivenText(random_file_name, real_text)\n",
    "\n",
    "    start = time.time()\n",
    "    os.remove(random_file_name)\n",
    "    print(\"Time for deleting file: \", str(time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    real_transcripts_ipa = \" \".join(\n",
    "        [word[0] for word in result[\"real_and_transcribed_words_ipa\"]]\n",
    "    )\n",
    "    matched_transcripts_ipa = \" \".join(\n",
    "        [word[1] for word in result[\"real_and_transcribed_words_ipa\"]]\n",
    "    )\n",
    "\n",
    "    real_transcripts = \" \".join(\n",
    "        [word[0] for word in result[\"real_and_transcribed_words\"]]\n",
    "    )\n",
    "    matched_transcripts = \" \".join(\n",
    "        [word[1] for word in result[\"real_and_transcribed_words\"]]\n",
    "    )\n",
    "\n",
    "    words_real = real_transcripts.lower().split()\n",
    "    mapped_words = matched_transcripts.split()\n",
    "\n",
    "    is_letter_correct_all_words = \"\"\n",
    "    for idx, word_real in enumerate(words_real):\n",
    "\n",
    "        mapped_letters, mapped_letters_indices = wm.get_best_mapped_words(\n",
    "            mapped_words[idx], word_real\n",
    "        )\n",
    "\n",
    "        is_letter_correct = wm.getWhichLettersWereTranscribedCorrectly(\n",
    "            word_real, mapped_letters\n",
    "        )  # , mapped_letters_indices)\n",
    "\n",
    "        is_letter_correct_all_words += (\n",
    "            \"\".join([str(is_correct) for is_correct in is_letter_correct]) + \" \"\n",
    "        )\n",
    "\n",
    "    pair_accuracy_category = \" \".join(\n",
    "        [str(category) for category in result[\"pronunciation_categories\"]]\n",
    "    )\n",
    "    print(\"Time to post-process results: \", str(time.time() - start))\n",
    "\n",
    "    res = {\n",
    "        \"real_transcript\": result[\"recording_transcript\"],\n",
    "        \"ipa_transcript\": result[\"recording_ipa\"],\n",
    "        \"pronunciation_accuracy\": str(int(result[\"pronunciation_accuracy\"])),\n",
    "        \"real_transcripts\": real_transcripts,\n",
    "        \"matched_transcripts\": matched_transcripts,\n",
    "        \"real_transcripts_ipa\": real_transcripts_ipa,\n",
    "        \"matched_transcripts_ipa\": matched_transcripts_ipa,\n",
    "        \"pair_accuracy_category\": pair_accuracy_category,\n",
    "        \"start_time\": result[\"start_time\"],\n",
    "        \"end_time\": result[\"end_time\"],\n",
    "        \"is_letter_correct_all_words\": is_letter_correct_all_words,\n",
    "    }\n",
    "\n",
    "    return json.dumps(res)\n",
    "\n",
    "\n",
    "# From Librosa\n",
    "\n",
    "\n",
    "def audioread_load(path, offset=0.0, duration=None, dtype=np.float32):\n",
    "    \"\"\"Load an audio buffer using audioread.\n",
    "\n",
    "    This loads one block at a time, and then concatenates the results.\n",
    "    \"\"\"\n",
    "\n",
    "    y = []\n",
    "    with audioread.audio_open(path) as input_file:\n",
    "        sr_native = input_file.samplerate\n",
    "        n_channels = input_file.channels\n",
    "\n",
    "        s_start = int(np.round(sr_native * offset)) * n_channels\n",
    "\n",
    "        if duration is None:\n",
    "            s_end = np.inf\n",
    "        else:\n",
    "            s_end = s_start + (int(np.round(sr_native * duration)) * n_channels)\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        for frame in input_file:\n",
    "            frame = buf_to_float(frame, dtype=dtype)\n",
    "            n_prev = n\n",
    "            n = n + len(frame)\n",
    "\n",
    "            if n < s_start:\n",
    "                # offset is after the current frame\n",
    "                # keep reading\n",
    "                continue\n",
    "\n",
    "            if s_end < n_prev:\n",
    "                # we're off the end.  stop reading\n",
    "                break\n",
    "\n",
    "            if s_end < n:\n",
    "                # the end is in this frame.  crop.\n",
    "                frame = frame[: s_end - n_prev]\n",
    "\n",
    "            if n_prev <= s_start <= n:\n",
    "                # beginning is in this frame\n",
    "                frame = frame[(s_start - n_prev) :]\n",
    "\n",
    "            # tack on the current frame\n",
    "            y.append(frame)\n",
    "\n",
    "    if y:\n",
    "        y = np.concatenate(y)\n",
    "        if n_channels > 1:\n",
    "            y = y.reshape((-1, n_channels)).T\n",
    "    else:\n",
    "        y = np.empty(0, dtype=dtype)\n",
    "\n",
    "    return y, sr_native\n",
    "\n",
    "\n",
    "# From Librosa\n",
    "\n",
    "\n",
    "def buf_to_float(x, n_bytes=2, dtype=np.float32):\n",
    "    \"\"\"Convert an integer buffer to floating point values.\n",
    "    This is primarily useful when loading integer-valued wav data\n",
    "    into numpy arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray [dtype=int]\n",
    "        The integer-valued data buffer\n",
    "\n",
    "    n_bytes : int [1, 2, 4]\n",
    "        The number of bytes per sample in ``x``\n",
    "\n",
    "    dtype : numeric type\n",
    "        The target output type (default: 32-bit float)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_float : np.ndarray [dtype=float]\n",
    "        The input data buffer cast to floating point\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert the scale of the data\n",
    "    scale = 1.0 / float(1 << ((8 * n_bytes) - 1))\n",
    "\n",
    "    # Construct the format string\n",
    "    fmt = \"<i{:d}\".format(n_bytes)\n",
    "\n",
    "    # Rescale and format the data buffer\n",
    "    return scale * np.frombuffer(x, fmt).astype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for saving binary in file:  0.0006592273712158203\n",
      "Time for loading .ogg file file:  0.0007236003875732422\n",
      "[NeMo I 2024-05-29 18:20:38 nemo_logging:381] Changed decoding strategy to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: true\n",
      "    compute_timestamps: true\n",
      "    word_seperator: ' '\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg: null\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: 0.33\n",
      "    temperature: 1.0\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b1d856bcf4447d96cefe69cba80e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:epitran:lex_lookup (from flite) is not installed.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mlambda_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_event\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 55\u001b[0m, in \u001b[0;36mlambda_handler\u001b[0;34m(event, result)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# start = time.time()\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# signal, fs = audioread_load(random_file_name)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# signal = transform(torch.Tensor(signal)).unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime for loading .ogg file file: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n\u001b[0;32m---> 55\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_SST_lambda\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessAudioForGivenText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     58\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(random_file_name)\n",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m, in \u001b[0;36mPronunciationTrainer.processAudioForGivenText\u001b[0;34m(self, recordedAudio, real_text)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessAudioForGivenText\u001b[39m(\u001b[38;5;28mself\u001b[39m, recordedAudio: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, real_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 45\u001b[0m     recording_transcript, recording_ipa, word_locations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetAudioTranscript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecordedAudio\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(recording_ipa)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime for NN to transcript audio: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n",
      "Cell \u001b[0;32mIn[3], line 94\u001b[0m, in \u001b[0;36mPronunciationTrainer.getAudioTranscript\u001b[0;34m(self, recordedAudio)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masr_model\u001b[38;5;241m.\u001b[39mprocessAudio(recordedAudio)\n\u001b[1;32m     91\u001b[0m current_recorded_transcript, current_recorded_word_locations \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetTranscriptAndWordsLocations(recordedAudio)\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 94\u001b[0m current_recorded_ipa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipa_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvertToPhonem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_recorded_transcript\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     99\u001b[0m     current_recorded_transcript,\n\u001b[1;32m    100\u001b[0m     current_recorded_ipa,\n\u001b[1;32m    101\u001b[0m     current_recorded_word_locations,\n\u001b[1;32m    102\u001b[0m )\n",
      "File \u001b[0;32m~/yashpratap/packages/ai-trainer/RuleBasedModels.py:18\u001b[0m, in \u001b[0;36mEpitranPhonemConverter.convertToPhonem\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvertToPhonem\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     phonem_representation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepitran_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransliterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m phonem_representation\n",
      "File \u001b[0;32m/opt/conda/envs/testing/lib/python3.11/site-packages/epitran/_epitran.py:63\u001b[0m, in \u001b[0;36mEpitran.transliterate\u001b[0;34m(self, word, normpunc, ligatures)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransliterate\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, normpunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ligatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transliterates/transcribes a word into IPA\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m        unicode: IPA string\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransliterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormpunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mligatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/testing/lib/python3.11/site-packages/epitran/flite.py:96\u001b[0m, in \u001b[0;36mFlite.transliterate\u001b[0;34m(self, text, normpunc, ligatures)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_re\u001b[38;5;241m.\u001b[39mfindall(text):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mletter_re\u001b[38;5;241m.\u001b[39mmatch(chunk):\n\u001b[0;32m---> 96\u001b[0m         acc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menglish_g2p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m         acc\u001b[38;5;241m.\u001b[39mappend(chunk)\n",
      "File \u001b[0;32m/opt/conda/envs/testing/lib/python3.11/site-packages/epitran/flite.py:214\u001b[0m, in \u001b[0;36mFliteLexLookup.english_g2p\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    211\u001b[0m     arpa_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Split on newlines and take the first element (in case lex_lookup\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# returns multiple lines).\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m arpa_text \u001b[38;5;241m=\u001b[39m \u001b[43marpa_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marpa_to_ipa(arpa_text)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "res = lambda_handler(dummy_event, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"real_transcript\": \"printing in the only zens with which we are at present concerned differs from most if not from altar arts and crafts represented in the exhibits zone\", \"ipa_transcript\": \"p\\u0280\\u026anti\\u02d0\\u014b i\\u02d0n te\\u02d0 \\u0254nly\\u02d0 t\\u0361s\\u0259ns vi\\u02d0t vhi\\u02d0x ve\\u02d0 \\u0251\\u02d0\\u0280e\\u02d0 \\u0251\\u02d0t p\\u0280e\\u02d0s\\u0259nt k\\u0254\\u014bk\\u0259\\u0280ne\\u02d0d d\\u026af\\u0259\\u0280s f\\u0280o\\u02d0m m\\u0254st i\\u02d0f no\\u02d0t f\\u0280o\\u02d0m alt\\u0251\\u02d0\\u0280 a\\u0280ts and k\\u0280afts \\u0280\\u025bp\\u0280e\\u02d0s\\u0259nte\\u02d0d i\\u02d0n te\\u02d0 \\u025bkshi\\u02d0b\\u026ats t\\u0361so\\u02d0n\\u0259\", \"pronunciation_accuracy\": \"91\", \"real_transcripts\": \"Printing in the only sense with which we are at present concerned differs from most if not from all the arts and crafts represented in the Exhibition\", \"matched_transcripts\": \"printing in the only zens with which we are at present concerned differs from most if not from - - arts and crafts represented in the exhibits\", \"real_transcripts_ipa\": \"p\\u0280\\u026ant\\u026a\\u014bk i\\u02d0n t\\u0259 \\u0254nly\\u02d0 z\\u0259ns\\u0259 vi\\u02d0t vhi\\u02d0x v\\u0259 \\u0251\\u02d0\\u0280\\u0259 \\u0251\\u02d0t p\\u0280e\\u02d0s\\u0259nt k\\u0254\\u014bk\\u0259\\u0280ne\\u02d0t d\\u026af\\u0259\\u0280s f\\u0280o\\u02d0m m\\u0254st i\\u02d0f no\\u02d0t f\\u0280o\\u02d0m al t\\u0259 a\\u0280ts ant k\\u0280afts \\u0280\\u025bp\\u0280e\\u02d0s\\u0259nte\\u02d0t i\\u02d0n t\\u0259 \\u025bkshi\\u02d0bi\\u02d0t\\u0361si\\u02d0o\\u02d0n\", \"matched_transcripts_ipa\": \"p\\u0280\\u026ant\\u026a\\u014bk i\\u02d0n t\\u0259 \\u0254nly\\u02d0 t\\u0361s\\u0259ns vi\\u02d0t vhi\\u02d0x v\\u0259 \\u0251\\u02d0\\u0280\\u0259 \\u0251\\u02d0t p\\u0280e\\u02d0s\\u0259nt k\\u0254\\u014bk\\u0259\\u0280ne\\u02d0t d\\u026af\\u0259\\u0280s f\\u0280o\\u02d0m m\\u0254st i\\u02d0f no\\u02d0t f\\u0280o\\u02d0m - - a\\u0280ts ant k\\u0280afts \\u0280\\u025bp\\u0280e\\u02d0s\\u0259nte\\u02d0t i\\u02d0n t\\u0259 \\u025bkshi\\u02d0b\\u026ats\", \"pair_accuracy_category\": \"0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0\", \"start_time\": \"0.16 0.56 0.72 0.88 1.04 1.44 1.84 2.08 2.32 2.56 3.04 3.44 4.16 4.64 4.88 5.04 5.44 5.68 9.36 9.36 6.88 7.28 7.36 7.84 8.48 8.64 8.88\", \"end_time\": \"0.56 0.72 0.88 1.04 1.44 1.84 2.08 2.32 2.56 3.04 3.44 4.16 4.64 4.88 5.04 5.44 5.68 6.0 9.6 9.6 7.28 7.36 7.84 8.48 8.64 8.88 9.36\", \"is_letter_correct_all_words\": \"11111111 11 111 1111 01110 1111 11111 11 111 11 1111111 111111111 1111111 1111 1111 11 111 1111 000 000 1111 111 111111 11111111111 11 111 1111111000 \"}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"real_transcript\": \"printing in the only zens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  {\"real_transcript\": \"printing in the only zens..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming res is your dictionary\n",
    "df = pd.DataFrame([res])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ace-tools (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ace-tools\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m     20\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall ace-tools\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPronunciation Data\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mdf)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data provided by the user\n",
    "data = {\n",
    "    \"real_transcript\": \"printing in the only sense with which we are at present callsanu differs from most if not from all artan and charaafter represented in achigisson\",\n",
    "    \"ipa_transcript\": \"ˈprɪnɪŋ ɪn ðə ˈoʊnli sɛns wɪθ wɪʃ wi ər æt ˈprɛzənt callsanu ˈdɪfərz frəm moʊst ɪf nɑt frəm ɔl artan ənd charaafter ˌrɛprɪˈzɛnɪd ɪn achigisson\",\n",
    "    \"pronunciation_accuracy\": 49,\n",
    "    \"real_transcripts\": \"Printing in the only sense with which we are at present concerned differs from most if not from all the arts and crafts represented in the Exhibition\",\n",
    "    \"matched_transcripts\": \"printing in the only sense with which we are at present callsanu differs from most if not from all artan and charaafter represented in achigisson - -\",\n",
    "    \"real_transcripts_ipa\": \"ˈprɪnɪŋ ɪn ðə ˈoʊnli sɛns wɪθ wɪʃ wi ər æt ˈprɛzənt kənˈsɜrnd ˈdɪfərz frəm moʊst ɪf nɑt frəm ɔl ðə ɑrts ənd kræfts ˌrɛprɪˈzɛnɪd ɪn ðə ˌɛksəˈbɪʃən\",\n",
    "    \"matched_transcripts_ipa\": \"ˈprɪnɪŋ ɪn ðə ˈoʊnli sɛns wɪθ wɪʃ wi ər æt ˈprɛzənt callsanu ˈdɪfərz frəm moʊst ɪf nɑt frəm ɔl artan ənd charaafter ˌrɛprɪˈzɛnɪd ɪn achigisson  \",\n",
    "    \"pair_accuracy_category\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    \"start_time\": [0.72, 1.2, 1.36, 1.52, 1.68, 2.16, 2.64, 2.8000000000000003, 3.04, 3.2, 3.44, 3.6, 4.72, 5.68, 5.92, 6.16, 6.640000000000001, 6.96, 7.28, 7.5200000000000005, 8.24, 8.4, 9.040000000000001, 10.8, 10.96, 10.96, 10.96],\n",
    "    \"end_time\": [1.2, 1.36, 1.52, 1.68, 2.16, 2.64, 2.8000000000000003, 3.04, 3.2, 3.44, 3.6, 4.72, 5.68, 5.92, 6.16, 6.640000000000001, 6.96, 7.28, 7.5200000000000005, 8.24, 8.4, 9.040000000000001, 10.8, 10.96, 11.92, 11.92, 11.92],\n",
    "    \"is_letter_correct_all_words\": \"11111111 11 111 1111 11111 1111 11111 11 111 11 1111111 100000000 1111000 1111 1111 11 111 1111 111 100 1000 100 010000 00000000000 11 000 0000000000\"\n",
    "}\n",
    "\n",
    "# Convert the data into a DataFrame for better visualization\n",
    "df = pd.DataFrame(data)\n",
    "%pip install ace-tools\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Pronunciation Data\", dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = 'Printing in the only sense with which we are at present concerned differs from most if not from all the arts and crafts represented in the Exhibition'\n",
    "recorded = 'printing in the only sense with which we are at present callsanu differs from most if not from all artan and charaafter represented in achigisson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_real = real.split()\n",
    "words_estimated = recorded.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  6.  7.  7.  7.  6.  7.  8.  7.  7.  5.  8.  7.  7.  7.  7.  7.  7.\n",
      "   8.  7.  6.  7.  6.  8.  6.  7.  7.]\n",
      " [ 6.  0.  3.  3.  4.  3.  4.  2.  3.  2.  6.  8.  6.  4.  4.  1.  3.  4.\n",
      "   3.  3.  4.  2.  6. 10.  0.  3.  8.]\n",
      " [ 7.  3.  0.  4.  4.  3.  4.  2.  2.  3.  6.  8.  6.  4.  4.  3.  3.  4.\n",
      "   3.  0.  4.  3.  6. 10.  3.  0.  9.]\n",
      " [ 7.  3.  4.  0.  4.  4.  5.  4.  4.  4.  7.  7.  7.  4.  4.  4.  3.  4.\n",
      "   3.  4.  4.  3.  6. 10.  3.  4. 10.]\n",
      " [ 7.  4.  4.  4.  0.  5.  5.  4.  4.  5.  5.  7.  6.  5.  4.  5.  4.  5.\n",
      "   5.  4.  4.  4.  6.  7.  4.  4. 10.]\n",
      " [ 6.  3.  3.  4.  5.  0.  2.  3.  4.  3.  7.  9.  6.  4.  4.  3.  3.  4.\n",
      "   4.  3.  3.  4.  5. 10.  3.  3.  8.]\n",
      " [ 7.  4.  4.  5.  5.  2.  0.  4.  5.  5.  7.  8.  7.  5.  5.  4.  5.  5.\n",
      "   5.  4.  5.  5.  6. 11.  4.  4.  8.]\n",
      " [ 8.  2.  2.  4.  4.  3.  4.  0.  2.  2.  6.  8.  6.  4.  4.  2.  3.  4.\n",
      "   3.  2.  4.  3.  6. 10.  2.  2. 10.]\n",
      " [ 7.  3.  2.  4.  4.  4.  5.  2.  0.  2.  5.  7.  6.  3.  4.  3.  3.  3.\n",
      "   2.  2.  2.  2.  5.  9.  3.  2. 10.]\n",
      " [ 7.  2.  3.  4.  5.  3.  5.  2.  2.  0.  6.  9.  7.  4.  3.  2.  2.  4.\n",
      "   2.  3.  2.  2.  4. 10.  2.  3.  9.]\n",
      " [ 6.  6.  6.  7.  5.  7.  7.  6.  5.  6.  0.  7.  6.  6.  5.  7.  6.  6.\n",
      "   7.  6.  5.  6.  6.  4.  6.  6.  9.]\n",
      " [ 7.  7.  8.  7.  7.  8.  8.  8.  7.  7.  6.  7.  8.  8.  7.  8.  8.  8.\n",
      "   5.  8.  6.  6.  6.  9.  7.  8. 10.]\n",
      " [ 7.  6.  6.  7.  6.  6.  7.  6.  6.  7.  6.  7.  0.  6.  7.  5.  7.  6.\n",
      "   7.  6.  6.  7.  5. 10.  6.  6.  9.]\n",
      " [ 7.  4.  4.  4.  5.  4.  5.  4.  3.  4.  6.  8.  6.  0.  4.  4.  3.  0.\n",
      "   4.  4.  3.  4.  5. 10.  4.  4.  9.]\n",
      " [ 7.  4.  4.  4.  4.  4.  5.  4.  4.  3.  5.  8.  7.  4.  0.  4.  2.  4.\n",
      "   4.  4.  4.  4.  5.  9.  4.  4.  9.]\n",
      " [ 7.  1.  3.  4.  5.  3.  4.  2.  3.  2.  7.  9.  5.  4.  4.  0.  3.  4.\n",
      "   3.  3.  4.  3.  5. 11.  1.  3.  9.]\n",
      " [ 7.  3.  3.  3.  4.  3.  5.  3.  3.  2.  6.  8.  7.  3.  2.  3.  0.  3.\n",
      "   3.  3.  3.  3.  5. 10.  3.  3.  9.]\n",
      " [ 7.  4.  4.  4.  5.  4.  5.  4.  3.  4.  6.  8.  6.  0.  4.  4.  3.  0.\n",
      "   4.  4.  3.  4.  5. 10.  4.  4.  9.]\n",
      " [ 8.  3.  3.  3.  5.  4.  5.  3.  2.  2.  7.  9.  7.  4.  4.  3.  3.  4.\n",
      "   0.  3.  3.  2.  5. 11.  3.  3. 10.]\n",
      " [ 5.  4.  4.  5.  5.  4.  5.  5.  3.  3.  5.  8.  7.  4.  5.  5.  4.  4.\n",
      "   4.  4.  2.  4.  5.  9.  4.  4.  8.]\n",
      " [ 7.  2.  3.  3.  4.  4.  5.  3.  2.  2.  6.  7.  7.  4.  4.  3.  3.  4.\n",
      "   2.  3.  3.  0.  5.  9.  2.  3. 10.]\n",
      " [ 9. 10.  8. 10.  9.  9.  9.  9.  7.  8.  9.  8.  8.  9.  9.  9.  9.  9.\n",
      "   9.  8.  7.  9.  5.  8. 10.  8. 10.]\n",
      " [ 9. 10. 10. 10.  7. 10. 11. 10.  9. 10.  4.  7. 10. 10.  9. 11. 10. 10.\n",
      "  11. 10.  9.  9.  9.  0. 10. 10. 11.]\n",
      " [ 6.  0.  3.  3.  4.  3.  4.  2.  3.  2.  6.  8.  6.  4.  4.  1.  3.  4.\n",
      "   3.  3.  4.  2.  6. 10.  0.  3.  8.]\n",
      " [ 9.  8.  9. 10.  9.  9.  8. 10.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n",
      "   9.  9.  8.  9.  8. 11.  8.  9.  5.]\n",
      " [ 8.  2.  3.  4.  5.  4.  5.  2.  3.  2.  7.  9.  7.  4.  4.  2.  3.  4.\n",
      "   3.  3.  4.  3.  6. 11.  2.  3. 10.]]\n"
     ]
    }
   ],
   "source": [
    "import WordMatching as wm\n",
    "import WordMetrics\n",
    "from ortools.sat.python import cp_model\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from dtwalign import dtw_from_distance_matrix\n",
    "import time\n",
    "\n",
    "offset_blank = 1\n",
    "TIME_THRESHOLD_MAPPING = 5.0\n",
    "\n",
    "\n",
    "word_distance_matrix = wm.get_word_distance_matrix(words_estimated, words_real)\n",
    "print(word_distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_path_from_distance_matrix(word_distance_matrix):\n",
    "    modelCpp = cp_model.CpModel()\n",
    "    print(\"check cp\")\n",
    "    number_of_real_words = word_distance_matrix.shape[1]\n",
    "    number_of_estimated_words = word_distance_matrix.shape[0]-1\n",
    "\n",
    "    number_words = np.maximum(number_of_real_words, number_of_estimated_words)\n",
    "\n",
    "    estimated_words_order = [modelCpp.NewIntVar(0, int(\n",
    "        number_words - 1 + offset_blank), 'w%i' % i) for i in range(number_words+offset_blank)]\n",
    "\n",
    "    # They are in ascending order\n",
    "    for word_idx in range(number_words-1):\n",
    "        modelCpp.Add(\n",
    "            estimated_words_order[word_idx+1] >= estimated_words_order[word_idx])\n",
    "\n",
    "    total_phoneme_distance = 0\n",
    "    real_word_at_time = {}\n",
    "    for idx_estimated in range(number_of_estimated_words):\n",
    "        for idx_real in range(number_of_real_words):\n",
    "            real_word_at_time[idx_estimated, idx_real] = modelCpp.NewBoolVar(\n",
    "                'real_word_at_time'+str(idx_real)+'-'+str(idx_estimated))\n",
    "            modelCpp.Add(estimated_words_order[idx_estimated] == idx_real).OnlyEnforceIf(\n",
    "                real_word_at_time[idx_estimated, idx_real])\n",
    "            total_phoneme_distance += word_distance_matrix[idx_estimated,\n",
    "                                                           idx_real]*real_word_at_time[idx_estimated, idx_real]\n",
    "\n",
    "    # If no word in time, difference is calculated from empty string\n",
    "    for idx_real in range(number_of_real_words):\n",
    "        word_has_a_match = modelCpp.NewBoolVar(\n",
    "            'word_has_a_match'+str(idx_real))\n",
    "        modelCpp.Add(sum([real_word_at_time[idx_estimated, idx_real] for idx_estimated in range(\n",
    "            number_of_estimated_words)]) == 1).OnlyEnforceIf(word_has_a_match)\n",
    "        total_phoneme_distance += word_distance_matrix[number_of_estimated_words,\n",
    "                                                       idx_real]*word_has_a_match.Not()\n",
    "\n",
    "    # Loss should be minimized\n",
    "    modelCpp.Minimize(total_phoneme_distance)\n",
    "\n",
    "    solver = cp_model.CpSolver()\n",
    "    solver.parameters.max_time_in_seconds = TIME_THRESHOLD_MAPPING\n",
    "    status = solver.Solve(modelCpp)\n",
    "    print('check 2')\n",
    "    print(status)\n",
    "    print(solver)\n",
    "    mapped_indices = []\n",
    "    try:\n",
    "        for word_idx in range(number_words):\n",
    "            mapped_indices.append(\n",
    "                (solver.Value(estimated_words_order[word_idx])))\n",
    "        print(\"checkkkk\")\n",
    "        return np.array(mapped_indices, dtype=int)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check cp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check 2\n",
      "4\n",
      "<ortools.sat.python.cp_model.CpSolver object at 0x7f99c9150510>\n",
      "checkkkk\n"
     ]
    }
   ],
   "source": [
    "mapped_indices = get_best_path_from_distance_matrix(word_distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 26 26 26]\n"
     ]
    }
   ],
   "source": [
    "print(mapped_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'həˈreɪ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eng_to_ipa as ipa\n",
    "ipa.convert(\"hurray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
